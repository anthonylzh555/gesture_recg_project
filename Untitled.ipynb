{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pdb, os, sys\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='teacher-student model')\n",
    "    parser.add_argument('--model', dest='model', help=\"model_path to save the student model\\n In testing, give trained student model.\", type=str)\n",
    "    parser.add_argument('--task', dest='task', help='task for this file, train/test/val', type=str)\n",
    "    parser.add_argument('--lr', dest='lr', default=1e-3, help='learning rate', type=float)\n",
    "    parser.add_argument('--epoch', dest='epoch', default=100, help='total epoch', type=int)\n",
    "    parser.add_argument('--dropout', dest='dropout', default=0.5, help=\"dropout ratio\", type=float)\n",
    "    parser.add_argument('--noisy', action='store_true', help='add noisy to logits (noisy-teacher model')\n",
    "    parser.add_argument('--noisy_ratio', dest='Nratio', default=0.5, help=\"noisy ratio\", type=float)\n",
    "    parser.add_argument('--noisy_sigma', dest='Nsigma', default=0.9, help=\"noisy sigma\", type=float)\n",
    "    parser.add_argument('--KD', action='store_true', help='knowledge distilling, hinton 2014')\n",
    "    parser.add_argument('--lamda', dest='lamda', default=0.3, help='KD method. lamda between original loss and soft-target loss.', type=float)\n",
    "    parser.add_argument('--tau', dest='tau', default=3.0, help='KD method. tau stands for temperature.', type=float)\n",
    "    parser.add_argument('--batchsize', dest='batchsize', default=256, type=int)\n",
    "    if len(sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global args, parser\n",
    "args, parser = parse_args()\n",
    "\n",
    "class bcolors:\n",
    "    END  = '\\033[0m'  # white (normal)\n",
    "    R  = '\\033[31m' # red\n",
    "    G  = '\\033[32m' # green\n",
    "    O  = '\\033[33m' # orange\n",
    "    B  = '\\033[34m' # blue\n",
    "    P  = '\\033[35m' # purple\n",
    "    BOLD = '\\033[1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of CNN model reference: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "# Create some wrappers for simplicity\n",
    "def conv(x, W, b, strides=1, padding='SAME'):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    #x = tf.pad(x, paddings, \"CONSTANT\")\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "def maxpool2d(x, k, s, padding='SAME'):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1],\n",
    "                          padding=padding)\n",
    "\n",
    "def avgpool2d(x, k, s, padding='SAME'):\n",
    "    # AvgPool2D wrapper\n",
    "    return tf.nn.avg_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1],\n",
    "                          padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=args.batchsize\n",
    "dim=32\n",
    "n_classes=10\n",
    "# placeholders\n",
    "global x,y,keep_prob\n",
    "\n",
    "# Create model\n",
    "def nin(): # modify from model - network in network\n",
    "\n",
    "    # pre-trained weight\n",
    "    npyfile = np.load('teacher.npy')\n",
    "    npyfile = npyfile.item()\n",
    "\n",
    "    weights = {\n",
    "        'conv1': tf.Variable(npyfile['conv1']['weights'], trainable=False, name = 'conv1_w'),\n",
    "        'cccp1': tf.Variable(npyfile['cccp1']['weights'], trainable=False, name = 'cccp1_w'),\n",
    "        'cccp2': tf.Variable(npyfile['cccp2']['weights'], trainable=False, name = 'cccp2_w'),\n",
    "        'conv2': tf.Variable(npyfile['conv2']['weights'], trainable=False, name = 'conv2_w'),\n",
    "        'cccp3': tf.Variable(npyfile['cccp3']['weights'], trainable=False, name = 'cccp3_w'),\n",
    "        'cccp4': tf.Variable(npyfile['cccp4']['weights'], trainable=False, name = 'cccp4_w'),\n",
    "        'conv3': tf.Variable(npyfile['conv3']['weights'], trainable=False, name = 'conv3_w'),\n",
    "        'cccp5': tf.Variable(npyfile['cccp5']['weights'], trainable=False, name = 'cccp5_w'),\n",
    "        'ip1': tf.Variable(npyfile['ip1']['weights'], trainable=False, name = 'ip1_w'),\n",
    "        'ip2': tf.Variable(npyfile['ip2']['weights'], trainable=False, name = 'ip2_w')\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'conv1': tf.Variable(npyfile['conv1']['biases'], trainable=False, name = 'conv1_b'),\n",
    "        'cccp1': tf.Variable(npyfile['cccp1']['biases'], trainable=False, name = 'cccp1_b'),\n",
    "        'cccp2': tf.Variable(npyfile['cccp2']['biases'], trainable=False, name = 'cccp2_b'),\n",
    "        'conv2': tf.Variable(npyfile['conv2']['biases'], trainable=False, name = 'conv2_b'),\n",
    "        'cccp3': tf.Variable(npyfile['cccp3']['biases'], trainable=False, name = 'cccp3_b'),\n",
    "        'cccp4': tf.Variable(npyfile['cccp4']['biases'], trainable=False, name = 'cccp4_b'),\n",
    "        'conv3': tf.Variable(npyfile['conv3']['biases'], trainable=False, name = 'conv3_b'),\n",
    "        'cccp5': tf.Variable(npyfile['cccp5']['biases'], trainable=False, name = 'cccp5_b'),\n",
    "        'ip1': tf.Variable(npyfile['ip1']['biases'], trainable=False, name = 'ip1_b'),\n",
    "        'ip2': tf.Variable(npyfile['ip2']['biases'], trainable=False, name = 'ip2_b')\n",
    "    }\n",
    "\n",
    "    conv1 = conv(x, weights['conv1'], biases['conv1'])\n",
    "    conv1_relu = tf.nn.relu(conv1)\n",
    "    cccp1 = conv(conv1_relu, weights['cccp1'], biases['cccp1'])\n",
    "    cccp1_relu = tf.nn.relu(cccp1)\n",
    "    cccp2 = conv(cccp1_relu, weights['cccp2'], biases['cccp2'])\n",
    "    cccp2_relu = tf.nn.relu(cccp2)\n",
    "    pool1 = maxpool2d(cccp2_relu, k=3, s=2)\n",
    "    drop3 = tf.nn.dropout(pool1, keep_prob)\n",
    "\n",
    "    conv2 = conv(drop3, weights['conv2'], biases['conv2'])\n",
    "    conv2_relu = tf.nn.relu(conv2)\n",
    "    cccp3 = conv(conv2_relu, weights['cccp3'], biases['cccp3'])\n",
    "    cccp3_relu = tf.nn.relu(cccp3)\n",
    "    cccp4 = conv(cccp3_relu, weights['cccp4'], biases['cccp4'])\n",
    "    cccp4_relu = tf.nn.relu(cccp4)\n",
    "\n",
    "    pool2 = avgpool2d(cccp4_relu, k=3, s=2)\n",
    "    drop6 = tf.nn.dropout(pool2, keep_prob)\n",
    "\n",
    "    conv3 = conv(drop6, weights['conv3'], biases['conv3'])\n",
    "    conv3_relu = tf.nn.relu(conv3)\n",
    "    cccp5 = conv(conv3_relu, weights['cccp5'], biases['cccp5'])\n",
    "    cccp5_relu = tf.nn.relu(cccp5)\n",
    "\n",
    "    # inner product\n",
    "    ip1 = tf.reshape(cccp5_relu, [-1, weights['ip1'].get_shape().as_list()[0]])\n",
    "    ip1 = tf.add(tf.matmul(ip1, weights['ip1']), biases['ip1'])\n",
    "    ip1_relu = tf.nn.relu(ip1)\n",
    "    ip2 = tf.add(tf.matmul(ip1_relu, weights['ip2']), biases['ip2'])\n",
    "\n",
    "    return ip2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(use_pretrained=False): # modify from lenet model\n",
    "\n",
    "    if use_pretrained == False:\n",
    "        # Random initialize\n",
    "        weights = {\n",
    "            'conv1': tf.get_variable('LN_conv1_w', [5,5,3,64],initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "            'conv2': tf.get_variable('LN_conv2_w', [5,5,64,128],initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "            'ip1': tf.get_variable('LN_ip1_w', [5*5*128, 1024] , initializer=tf.contrib.layers.xavier_initializer()),\n",
    "            'ip2': tf.get_variable('LN_ip2_w', [1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'conv1': tf.Variable(tf.random_normal(shape=[64],stddev=0.5), name = 'LN_conv1_b'),\n",
    "            'conv2': tf.Variable(tf.random_normal(shape=[128],stddev=0.5), name = 'LN_conv2_b'),\n",
    "            'ip1': tf.Variable(tf.random_normal(shape=[1024],stddev=0.5), name = 'LN_ip1_b'),\n",
    "            'ip2': tf.Variable(tf.random_normal(shape=[10],stddev=0.5), name = 'LN_ip2_b')\n",
    "        }\n",
    "    else:\n",
    "        # initialized by pre-trained weight\n",
    "        npyfile = np.load('student.npy')\n",
    "        npyfile = npyfile.item()\n",
    "        weights = {\n",
    "            'conv1': tf.Variable(npyfile['conv1']['weights'], name = 'LN_conv1_w'),\n",
    "            'conv2': tf.Variable(npyfile['conv2']['weights'], name = 'LN_conv2_w'),\n",
    "            'ip1': tf.Variable(npyfile['ip1']['weights'], name = 'LN_ip1_w'),\n",
    "            'ip2': tf.Variable(npyfile['ip2']['weights'], name = 'LN_ip2_w'),\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'conv1': tf.Variable(npyfile['conv1']['biases'], name = 'LN_conv1_b'),\n",
    "            'conv2': tf.Variable(npyfile['conv2']['biases'], name = 'LN_conv2_b'),\n",
    "            'ip1': tf.Variable(npyfile['ip1']['biases'], name = 'LN_ip1_b'),\n",
    "            'ip2': tf.Variable(npyfile['ip2']['biases'], name = 'LN_ip2_b'),\n",
    "        }\n",
    "\n",
    "    conv1 = conv(x, weights['conv1'], biases['conv1'],padding='VALID')\n",
    "    pool1 = maxpool2d(conv1, k=2, s=2)\n",
    "    conv2 = conv(pool1, weights['conv2'], biases['conv2'], padding='VALID')\n",
    "    pool2 = maxpool2d(conv2, k=2, s=2,padding='VALID')\n",
    "\n",
    "    ip1 = tf.reshape(pool2, [-1, weights['ip1'].get_shape().as_list()[0]])\n",
    "    ip1 = tf.add(tf.matmul(ip1, weights['ip1']), biases['ip1'])\n",
    "    ip1_relu = tf.nn.relu(ip1)\n",
    "    ip2 = tf.add(tf.matmul(ip1_relu, weights['ip2']), biases['ip2'])\n",
    "    return ip2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading cifar dataset\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def cal_mean():\n",
    "    data, label = read_cifar10('train')\n",
    "    mean = np.mean(data, axis=0)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def read_cifar10(flag):\n",
    "    # Directory of cifar-10\n",
    "    path = '/home/james/Desktop/workspace/model_compression/cifar-10-batches-py'\n",
    "    if flag == 'train':\n",
    "        batches = ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "        data = np.uint8(np.zeros((50000,32,32,3)))\n",
    "        label=np.int16(np.zeros((50000,1)))\n",
    "    elif flag == 'test':\n",
    "        batches = ['test_batch']\n",
    "        data = np.float32(np.zeros((10000,32,32,3)))\n",
    "        label= np.zeros((10000,1))\n",
    "\n",
    "    N = 10000\n",
    "    for i in xrange(len(batches)):\n",
    "        b = batches[i]\n",
    "        temp = unpickle(os.path.join(path,b))\n",
    "        for j in xrange(N):\n",
    "            data[N*i+j][:,:,2] = np.reshape(temp['data'][j][2048:],(32,32))\n",
    "            data[N*i+j][:,:,1] = np.reshape(temp['data'][j][1024:2048],(32,32))\n",
    "            data[N*i+j][:,:,0] = np.reshape(temp['data'][j][:1024],(32,32))\n",
    "            label[N*i+j] = temp['labels'][j]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    learning_rate=args.lr\n",
    "    model_path=args.model\n",
    "    total_epoch = args.epoch\n",
    "    teacher=nin()\n",
    "    student=lenet()\n",
    "    if args.noisy == True:\n",
    "        drop_scale = 1/args.Nratio\n",
    "        noisy_mask = tf.nn.dropout( tf.constant(np.float32(np.ones((batch_size,1)))/drop_scale) ,keep_prob=args.Nratio) #(batchsize,1)\n",
    "        gaussian = tf.random_normal(shape=[batch_size,1], mean=0.0, stddev=args.Nsigma)\n",
    "        noisy = tf.mul(noisy_mask, gaussian)\n",
    "        #noisy_add = tf.add(tf.constant(np.float32(np.ones((batch_size,1)))), noisy)\n",
    "        teacher = tf.mul(teacher, tf.tile(noisy,tf.constant([1,10])))   #(batchsize,10)\n",
    "        #teacher = tf.add(teacher, tf.tile(noisy,tf.constant([1,10])))\n",
    "        print bcolors.G+\"prepare for training, noisy mode\"+bcolors.END\n",
    "        tf_loss = tf.nn.l2_loss(teacher - student)/batch_size\n",
    "    elif args.KD == True:   # correct Hinton method at 2017.1.3\n",
    "        print bcolors.G+\"prepare for training, knowledge distilling mode\"+bcolors.END\n",
    "        one_hot = tf.one_hot(y, n_classes,1.0,0.0)\n",
    "        #one_hot = tf.cast(one_hot_int, tf.float32)\n",
    "        teacher_tau = tf.scalar_mul(1.0/args.tau, teacher)\n",
    "        student_tau = tf.scalar_mul(1.0/args.tau, student)\n",
    "        objective1 = tf.nn.sigmoid_cross_entropy_with_logits(student_tau, one_hot)\n",
    "        objective2 = tf.scalar_mul(0.5, tf.square(student_tau-teacher_tau))\n",
    "        tf_loss = (args.lamda*tf.reduce_sum(objective1) + (1-args.lamda)*tf.reduce_sum(objective2))/batch_size\n",
    "    else:\n",
    "        print bcolors.G+\"prepare for training, NIPS2014 mode\"+bcolors.END\n",
    "        tf_loss = tf.nn.l2_loss(teacher - student)/batch_size\n",
    "\n",
    "    optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(tf_loss)\n",
    "    optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate/10).minimize(tf_loss)\n",
    "\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True))\n",
    "    tf.initialize_all_variables().run()\n",
    "    with tf.device('/cpu:0'):\n",
    "        saver = tf.train.Saver(max_to_keep=100)\n",
    "        #saver.restore(sess, os.path.join(model_path,'model-99')\n",
    "    data, label=read_cifar10('train')\n",
    "    index=np.array(range(len(data)))    # index randomly ordered\n",
    "    mean = cal_mean()\n",
    "    begin = time.time()\n",
    "    iterations = len(data)/batch_size\n",
    "    decay_step = int(total_epoch*0.8)\n",
    "    cnt=0\n",
    "    dropout_rate=args.dropout\n",
    "    print bcolors.G+\"number of iterations (per epoch) =\"+str(len(data)/batch_size)+bcolors.END\n",
    "    for i in xrange(total_epoch):\n",
    "        np.random.shuffle(index)\n",
    "        cost_sum=0\n",
    "        for j in xrange(iterations):\n",
    "            batch_x = np.float32(data[index[j*batch_size:(j+1)*batch_size]]) - mean\n",
    "            batch_y = np.squeeze(np.float32(label[index[j*batch_size:(j+1)*batch_size]]))\n",
    "            if cnt/decay_step == 0:\n",
    "                lr=learning_rate\n",
    "                _, cost = sess.run([optimizer1, tf_loss],\n",
    "                    feed_dict={x : batch_x, y : batch_y, keep_prob : 1-dropout_rate})\n",
    "            elif cnt/decay_step == 1:\n",
    "                lr=learning_rate/10\n",
    "                _, cost = sess.run([optimizer2, tf_loss],\n",
    "                    feed_dict={x : batch_x, y : batch_y, keep_prob : 1-dropout_rate})\n",
    "            cost_sum += cost\n",
    "            #pdb.set_trace()\n",
    "            #if (j % int(iterations*0.25) == 0):\n",
    "            #    print (\"epoch %d-iter %d, cost = %f , avg-cost = %f\"%(i, j, cost, cost/n_classes))\n",
    "            #    sys.stdout.flush()\n",
    "        cnt +=1\n",
    "        avg_time = time.time()-begin\n",
    "        print (\"epoch %d - avg. %f seconds in each epoch, lr = %.0e, cost = %f , avg-cost-per-logits = %f\"%(i, avg_time/cnt,lr, cost_sum, cost_sum/iterations/n_classes))\n",
    "        if np.mod(i+1, 10) == 0:\n",
    "            print (\"Epoch \", i+1, \" is done. Saving the model ...\")\n",
    "            with tf.device('/cpu:0'):\n",
    "                if not os.path.exists(model_path):\n",
    "                    os.makedirs(model_path)\n",
    "                saver.save(sess, os.path.join(model_path, 'model'), global_step=i)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print bcolors.G+\"Task : test\\n\"+bcolors.END\n",
    "    student = lenet()\n",
    "    pred = tf.nn.softmax(student)\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    init=tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    with tf.device('/cpu:0'):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, args.model)\n",
    "\n",
    "    mean = cal_mean()\n",
    "    data, label=read_cifar10('test')\n",
    "    total=0\n",
    "    correct=0\n",
    "    begin = time.time()\n",
    "    for j in xrange(len(data)/batch_size):\n",
    "        batch_x = data[j*batch_size:(j+1)*batch_size] - mean\n",
    "        prob = sess.run([pred],\n",
    "                feed_dict={x : batch_x, y : np.ones((batch_size)), keep_prob : 1.0})\n",
    "        if np.argmax(prob[0]) == label[j]:\n",
    "            correct += 1\n",
    "        total+=1\n",
    "        #print (\"acc = %f . %d/%d\"%(float(correct)/total, correct, total))\n",
    "    end = time.time()\n",
    "    print (\"acc = %f . %d/%d.  Computing time = %f seconds\"%(float(correct)/total, correct, total, end-begin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_nin():\n",
    "    print bcolors.G+\"Task : val\\nvalidate the pre-trained nin model, should be same as caffe result\"+bcolors.END\n",
    "    pool3=nin()\n",
    "    #pool3 = lenet()\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    init=tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    mean = cal_mean()\n",
    "    data, label=read_cifar10('test')\n",
    "    total=0\n",
    "    correct=0\n",
    "    begin = time.time()\n",
    "    for j in xrange(len(data)/batch_size):\n",
    "        batch_x = data[j*batch_size:(j+1)*batch_size] - mean\n",
    "        prob = sess.run([pool3],\n",
    "                feed_dict={x : batch_x, y : np.ones((batch_size)), keep_prob : 1.0})\n",
    "        if np.argmax(prob[0]) == label[j]:\n",
    "            correct += 1\n",
    "        total+=1\n",
    "    end = time.time()\n",
    "    print (\"acc = %f . %d/%d.  Computing time = %f seconds\"%(float(correct)/total, correct, total, end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    global batch_size\n",
    "    print bcolors.G+\"Reading args....\"\n",
    "    print args\n",
    "    print bcolors.END\n",
    "    if args.noisy == True and args.KD == True:\n",
    "        print bcolors.BOLD+bcolors.R+\"Invalid args!\\n\"+bcolors.END+bcolors.R+\"only one method can be selected, noisy or KD(knowledge distilling)\"+bcolors.END\n",
    "        exit(1)\n",
    "    if args.task=='test' or args.task=='val':\n",
    "        batch_size=1\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [batch_size, dim, dim, 3])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    with tf.device('/gpu:0'):\n",
    "        if args.task=='train':\n",
    "            train()\n",
    "        elif args.task=='test':\n",
    "            test()\n",
    "        elif args.task=='val':\n",
    "            valid_nin()\n",
    "        else:\n",
    "            print bcolors.BOLD+bcolors.R+\"Invalid args!\\n\"+bcolors.END+bcolors.R+\"task should be train, test, or val\"+bcolors.END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
