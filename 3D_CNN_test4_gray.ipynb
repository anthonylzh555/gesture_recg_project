{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#import theano\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image specification\n",
    "img_rows,img_cols = 112,112\n",
    "img_depth = 16\n",
    "X_tr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import thumbup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [01:22<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 16)\n",
      "116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_folder_path = os.path.join(os.getcwd(), \"action_data/Hand/thumbup\")\n",
    "train_folder_list = sorted(os.listdir(train_folder_path))\n",
    "\n",
    "for dicname in tqdm(train_folder_list):\n",
    "    list_path = os.path.join(train_folder_path, dicname)\n",
    "    frames = []\n",
    "    i=0\n",
    "    for img in sorted(os.listdir(list_path)):\n",
    "        if i < img_depth:\n",
    "            img_path = os.path.join(list_path, img)\n",
    "            frame = cv2.imread(os.path.join(list_path, img))\n",
    "            frame = cv2.resize(frame,(img_rows,img_cols))\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)\n",
    "            i = i+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    input=np.array(frames)\n",
    "    #print (input.shape)\n",
    "    ipt = np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    #ipt=np.rollaxis(ipt,3,0)\n",
    "    #print (ipt.shape)\n",
    "    #print(\"__\")\n",
    "    X_tr.append(ipt)\n",
    "    \n",
    "print (ipt.shape)\n",
    "num_samples = len(X_tr)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [02:10<00:00,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 16)\n",
      "314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_folder_path = os.path.join(os.getcwd(), \"action_data/Hand/stop\")\n",
    "train_folder_list = sorted(os.listdir(train_folder_path))\n",
    "\n",
    "for dicname in tqdm(train_folder_list):\n",
    "    list_path = os.path.join(train_folder_path, dicname)\n",
    "    frames = []\n",
    "    i=0\n",
    "    for img in sorted(os.listdir(list_path)):\n",
    "        if i < img_depth:\n",
    "            img_path = os.path.join(list_path, img)\n",
    "            frame = cv2.imread(os.path.join(list_path, img))\n",
    "            frame = cv2.resize(frame,(img_rows,img_cols))\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)\n",
    "            i = i+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    input=np.array(frames)\n",
    "    #print (input.shape)\n",
    "    ipt = np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    #ipt=np.rollaxis(ipt,3,0)\n",
    "    #print (ipt.shape)\n",
    "    #print(\"__\")\n",
    "    X_tr.append(ipt)\n",
    "    \n",
    "print (ipt.shape)\n",
    "num_samples = len(X_tr)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [01:19<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 16)\n",
      "430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_folder_path = os.path.join(os.getcwd(), \"action_data/Hand/clockwise\")\n",
    "train_folder_list = sorted(os.listdir(train_folder_path))\n",
    "\n",
    "for dicname in tqdm(train_folder_list):\n",
    "    list_path = os.path.join(train_folder_path, dicname)\n",
    "    frames = []\n",
    "    i=0\n",
    "    for img in sorted(os.listdir(list_path)):\n",
    "        if i < img_depth:\n",
    "            img_path = os.path.join(list_path, img)\n",
    "            frame = cv2.imread(os.path.join(list_path, img))\n",
    "            frame = cv2.resize(frame,(img_rows,img_cols))\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)\n",
    "            i = i+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    input=np.array(frames)\n",
    "    #print (input.shape)\n",
    "    ipt = np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    #ipt=np.rollaxis(ipt,3,0)\n",
    "    #print (ipt.shape)\n",
    "    #print(\"__\")\n",
    "    X_tr.append(ipt)\n",
    "    \n",
    "print (ipt.shape)\n",
    "num_samples = len(X_tr)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import counterclockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [01:24<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 16)\n",
      "554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_folder_path = os.path.join(os.getcwd(), \"action_data/Hand/counterclockwise\")\n",
    "train_folder_list = sorted(os.listdir(train_folder_path))\n",
    "\n",
    "for dicname in tqdm(train_folder_list):\n",
    "    list_path = os.path.join(train_folder_path, dicname)\n",
    "    frames = []\n",
    "    i=0\n",
    "    for img in sorted(os.listdir(list_path)):\n",
    "        if i < img_depth:\n",
    "            img_path = os.path.join(list_path, img)\n",
    "            frame = cv2.imread(os.path.join(list_path, img))\n",
    "            frame = cv2.resize(frame,(img_rows,img_cols))\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)\n",
    "            i = i+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    input=np.array(frames)\n",
    "    #print (input.shape)\n",
    "    ipt = np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    #ipt=np.rollaxis(ipt,3,0)\n",
    "    #print (ipt.shape)\n",
    "    #print(\"__\")\n",
    "    X_tr.append(ipt)\n",
    "    \n",
    "print (ipt.shape)\n",
    "num_samples = len(X_tr)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import updown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [01:46<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 16)\n",
      "741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_folder_path = os.path.join(os.getcwd(), \"action_data/Hand/UpDown\")\n",
    "train_folder_list = sorted(os.listdir(train_folder_path))\n",
    "\n",
    "for dicname in tqdm(train_folder_list):\n",
    "    list_path = os.path.join(train_folder_path, dicname)\n",
    "    frames = []\n",
    "    i=0\n",
    "    for img in sorted(os.listdir(list_path)):\n",
    "        if i < img_depth:\n",
    "            img_path = os.path.join(list_path, img)\n",
    "            frame = cv2.imread(os.path.join(list_path, img))\n",
    "            frame = cv2.resize(frame,(img_rows,img_cols))\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)\n",
    "            i = i+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    input=np.array(frames)\n",
    "    #print (input.shape)\n",
    "    ipt = np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    #ipt=np.rollaxis(ipt,3,0)\n",
    "    #print (ipt.shape)\n",
    "    #print(\"__\")\n",
    "    X_tr.append(ipt)\n",
    "    \n",
    "print (ipt.shape)\n",
    "num_samples = len(X_tr)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741\n"
     ]
    }
   ],
   "source": [
    "X_tr_array = np.array(X_tr)   # convert the frames read into array\n",
    "\n",
    "num_samples = len(X_tr_array) \n",
    "print (num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train shape: (741, 112, 112, 16)\n"
     ]
    }
   ],
   "source": [
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:116]= 0                   #thumbup\n",
    "label[117:314] = 1                #stop\n",
    "label[315:430] = 2                #clockwise\n",
    "label[431:554] = 3                #counterclockwise\n",
    "label[555:741]= 4                 #updown\n",
    "\n",
    "train_data = [X_tr_array,label]\n",
    "\n",
    "(X_train, y_train) = (train_data[0],train_data[1])\n",
    "print('X_Train shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.zeros((num_samples, 1, img_rows,img_cols,img_depth))\n",
    "#train_set = X_train\n",
    "for h in range(num_samples):\n",
    "    train_set[h][0][:][:][:][:]=X_train[h,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(741, 112, 112, 16) X_train shapes\n",
      "(741, 1, 112, 112, 16) train samples\n"
     ]
    }
   ],
   "source": [
    "#patch_size = 16    # img_depth or number of frames used for each video\n",
    "\n",
    "print(X_train.shape,\"X_train shapes\")\n",
    "print(train_set.shape, 'train samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Pre-processing\\ntrain_set = train_set.astype('float32')\\ntrain_set -= np.mean(train_set)\\ntrain_set /= np.max(train_set)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classes = 5\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "'''\n",
    "# Pre-processing\n",
    "train_set = train_set.astype('float32')\n",
    "train_set -= np.mean(train_set)\n",
    "train_set /= np.max(train_set)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 64, 112, 112, 16)  1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 112, 112, 16)  256       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 64, 56, 56, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 128, 56, 56, 16)   221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 56, 56, 16)   512       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 128, 28, 28, 8)    0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 256, 28, 28, 8)    884992    \n",
      "_________________________________________________________________\n",
      "conv3b (Conv3D)              (None, 256, 28, 28, 8)    1769728   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256, 28, 28, 8)    1024      \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 256, 14, 14, 4)    0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 512, 14, 14, 4)    3539456   \n",
      "_________________________________________________________________\n",
      "conv4b (Conv3D)              (None, 512, 14, 14, 4)    7078400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512, 14, 14, 4)    2048      \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling3D)         (None, 512, 7, 7, 2)      0         \n",
      "_________________________________________________________________\n",
      "conv5a (Conv3D)              (None, 512, 7, 7, 2)      7078400   \n",
      "_________________________________________________________________\n",
      "conv5b (Conv3D)              (None, 512, 7, 7, 2)      7078400   \n",
      "_________________________________________________________________\n",
      "zeropad5 (ZeroPadding3D)     (None, 512, 8, 8, 2)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512, 8, 8, 2)      2048      \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling3D)         (None, 512, 4, 4, 1)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 78,018,693\n",
      "Trainable params: 78,015,749\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Dropout,Conv3D,Input,MaxPool3D\n",
    "from keras.layers import Flatten,Activation, BatchNormalization, ZeroPadding3D\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "import keras\n",
    "\n",
    "\n",
    "input_shape = (1,112,112,16) ##channel first ##channel,spatial_dim1, spatial_dim2, spatial_dim3,\n",
    "weight_decay = 0.005\n",
    "nb_classes = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer group\n",
    "model.add(Conv3D(64,(3,3,3),strides=(1,1,1),padding='same', input_shape= input_shape, name='conv1',data_format=\"channels_first\", activation='relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPool3D(pool_size=(2, 2, 1), strides=(2, 2, 1),padding='same', name='pool1',data_format=\"channels_first\"))\n",
    "\n",
    "# 2nd layer group\n",
    "model.add(Conv3D(128, (3, 3, 3),padding='same', activation='relu', name='conv2',data_format=\"channels_first\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2),padding='same', name='pool2',data_format=\"channels_first\"))\n",
    "\n",
    "# 3rd layer group\n",
    "model.add(Conv3D(256, (3, 3, 3),padding='same', activation='relu', name='conv3a',data_format=\"channels_first\"))\n",
    "model.add(Conv3D(256, (3, 3, 3),padding='same', activation='relu', name='conv3b',data_format=\"channels_first\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2),padding='same', name='pool3',data_format=\"channels_first\"))\n",
    "    \n",
    "# 4th layer group\n",
    "model.add(Conv3D(512, (3, 3, 3),padding='same', activation='relu', name='conv4a',data_format=\"channels_first\"))\n",
    "model.add(Conv3D(512, (3, 3, 3),padding='same', activation='relu', name='conv4b',data_format=\"channels_first\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2),padding='same', name='pool4',data_format=\"channels_first\"))\n",
    "    \n",
    "# 5th layer group\n",
    "model.add(Conv3D(512, (3, 3, 3),padding='same', activation='relu', name='conv5a',data_format=\"channels_first\"))\n",
    "model.add(Conv3D(512, (3, 3, 3),padding='same', activation='relu', name='conv5b',data_format=\"channels_first\"))\n",
    "model.add(ZeroPadding3D(padding=((0, 1), (0, 1), (0, 0)), name='zeropad5',data_format=\"channels_first\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2),padding='same', name='pool5',data_format=\"channels_first\"))\n",
    "model.add(Flatten())\n",
    "    \n",
    "# FC layers group\n",
    "model.add(Dense(4096, activation='relu', name='fc6'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu',kernel_initializer='glorot_normal', name='fc7'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax',kernel_regularizer=l2(weight_decay), name='fc8'))\n",
    "\n",
    "print(model.summary())\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_val_new, y_train_new,y_val_new =  train_test_split(train_set, Y_train, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train_new.astype('float32')\n",
    "X_train_new -= np.mean(X_train_new)\n",
    "X_train_new /= np.max(X_train_new)\n",
    "\n",
    "X_val_new = X_val_new.astype('float32')\n",
    "X_val_new -= np.mean(X_val_new)\n",
    "X_val_new /= np.max(X_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 592 samples, validate on 149 samples\n",
      "Epoch 1/100\n",
      "592/592 [==============================] - 39s 66ms/step - loss: 11.9559 - acc: 0.2382 - val_loss: 11.8360 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 11.83598, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 2/100\n",
      "592/592 [==============================] - 29s 49ms/step - loss: 12.1867 - acc: 0.2466 - val_loss: 11.8335 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00002: val_loss improved from 11.83598 to 11.83346, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 3/100\n",
      "592/592 [==============================] - 29s 50ms/step - loss: 12.1843 - acc: 0.2466 - val_loss: 11.8312 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00003: val_loss improved from 11.83346 to 11.83121, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 4/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1822 - acc: 0.2466 - val_loss: 11.8292 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00004: val_loss improved from 11.83121 to 11.82916, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 5/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1802 - acc: 0.2466 - val_loss: 11.8273 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00005: val_loss improved from 11.82916 to 11.82725, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 6/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1783 - acc: 0.2466 - val_loss: 11.8255 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00006: val_loss improved from 11.82725 to 11.82548, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 7/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1766 - acc: 0.2466 - val_loss: 11.8238 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00007: val_loss improved from 11.82548 to 11.82381, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 8/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1750 - acc: 0.2466 - val_loss: 11.8223 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00008: val_loss improved from 11.82381 to 11.82225, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 9/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1735 - acc: 0.2466 - val_loss: 11.8208 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00009: val_loss improved from 11.82225 to 11.82078, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 10/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1720 - acc: 0.2466 - val_loss: 11.8194 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00010: val_loss improved from 11.82078 to 11.81940, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 11/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1707 - acc: 0.2466 - val_loss: 11.8181 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00011: val_loss improved from 11.81940 to 11.81809, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 12/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1694 - acc: 0.2466 - val_loss: 11.8169 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00012: val_loss improved from 11.81809 to 11.81685, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 13/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1682 - acc: 0.2466 - val_loss: 11.8157 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00013: val_loss improved from 11.81685 to 11.81568, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 14/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1671 - acc: 0.2466 - val_loss: 11.8146 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00014: val_loss improved from 11.81568 to 11.81458, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 15/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1660 - acc: 0.2466 - val_loss: 11.8135 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00015: val_loss improved from 11.81458 to 11.81353, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 16/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1650 - acc: 0.2466 - val_loss: 11.8125 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00016: val_loss improved from 11.81353 to 11.81254, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 17/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1640 - acc: 0.2466 - val_loss: 11.8116 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00017: val_loss improved from 11.81254 to 11.81159, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 18/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1631 - acc: 0.2466 - val_loss: 11.8107 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00018: val_loss improved from 11.81159 to 11.81070, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n",
      "Epoch 19/100\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 12.1622 - acc: 0.2466 - val_loss: 11.8098 - val_acc: 0.2685\n",
      "\n",
      "Epoch 00019: val_loss improved from 11.81070 to 11.80985, saving model to /home/jovyan/project/saved_models/C3D_model_test4_gray.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4fab0ce95501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                  callbacks=[checkpoint, earlystop])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                         \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "nb_epoch = 100\n",
    "\n",
    "# Use ModelCheckpoint to save model and weights\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'C3D_model_test4_gray.h5'\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# earlystop\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "\n",
    "hist = model.fit(X_train_new, y_train_new, \n",
    "                 validation_data=(X_val_new,y_val_new),\n",
    "                 batch_size=batch_size,\n",
    "                 epochs = nb_epoch, \n",
    "                 shuffle=True, \n",
    "                 callbacks=[checkpoint, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our save model\n",
    "print(\"Loading trained model\")\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val_new, y_val_new, batch_size=batch_size)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "\n",
    "plt.figure(1,figsize=(7,5))\n",
    "plt.plot(train_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "print (plt.style.available) # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "plt.figure(2,figsize=(7,5))\n",
    "plt.plot(train_acc, label=\"training_acc\")\n",
    "plt.plot(val_acc, label=\"validation_acc\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "print (plt.style.available) # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def confusion_matrix_plot(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train_new, y_train_new)\n",
    "#y_pred = model.predict(X_val_new)\n",
    "y_pred = model.predict(train_set)\n",
    "\n",
    "#a = np.argmax(y_val_new, axis=1)\n",
    "a = np.argmax(Y_train, axis=1)\n",
    "b = np.argmax(y_pred, axis=1)\n",
    "print(\"benchmark : \",a)\n",
    "print(\"predict : \",b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met = confusion_matrix(a, b)\n",
    "np.set_printoptions(precision=2)\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_plot(met, normalize=True, classes=['thumbup', 'stop', 'clockwise', 'counterclockwise', 'updown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
